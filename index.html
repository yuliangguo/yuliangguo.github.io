<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yuliang Guo</title>

    <meta name="author" content="Yuliang Guo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Yuliang Guo
                </p>
                <p>I'm a Lead (Staff) Research Scientist at Bosch AI Research Center at Silicon Valley, where I lead a team that works on 3D vision and interactive AI solutions.
                </p>
                <p>
                    At Bosch, my work focuses on pioneering computer vision and AI research, aimed at enabling generalization across diverse hardware configurations and embodiments. My research has been integrated into several Bosch products or prototypes, such as assisted driving technologies, industrial augmented reality (AR) systems, and AI-powered indoor robotics.                
                </p>
                <p style="text-align:center">
                  <a href="mailto:33yuliangguo@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/YuliangGuo-CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/YuliangGuo-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=CP-YkUwAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/yuliangguo/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/YuliangGuo2021.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/YuliangGuo2021.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My research interests are centered on formulating generalized 3D perception methods adaptable across various embodiments, as well building interactable scene models from real world. My work aims to foster intelligent interactions between humans and AI while upholding safety in the physical world.                
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr onmouseout="upnerf_stop()" onmouseover="upnerf_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='upnerf_image'><video  width=100% muted autoplay loop>
            <source src="TODO" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/upnerf_demo.gif' width=95%>
            </div>
            <script type="text/javascript">
            function upnerf_start() {
                document.getElementById('upnerf_image').style.opacity = "1";
            }
    
            function upnerf_stop() {
                document.getElementById('upnerf_image').style.opacity = "0";
            }
            upnerf_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://yuliangguo.github.io/upnerf/">
            <span class="papertitle">UPNeRF: A Unified Framework for Monocular 3D Object Reconstruction and Pose Estimation</span>
            </a>
            <br>
            <strong>Yuliang Guo</strong>,
            Abhinav Kumar,
            Cheng Zhao,
            Ruoyu Wang,
            Xinyu Huang,
            Liu Ren,
            <br>
            <em>arXiv</em> 2024
            <br>
            [<a href="https://arxiv.org/abs/2403.15705">paper</a>]
            <!-- [<a href="https://github.com/yuliangguo/nerf-auto-driving">code</a>] -->
            [<a href="https://yuliangguo.github.io/upnerf/">project page</a>]
            <p></p>
            <p>
                A monocular object reconstruction framework effectively integrating object pose estimation and NeRF-based reconstruction. A novel pose estimation is introduced to handle depth-scale ambiguity and cross-domain generalization.          
            </p>
        </td>
        </tr>


    <tr onmouseout="seabird_stop()" onmouseover="seabird_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='seabird_image'><video  width=100% muted autoplay loop>
            <source src="images/seabird_demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/Seabird_teasor.gif' width=100%>
            <!-- <img src='images/Seabird_teaser_nuscenes.png' width=100%> -->
            </div>
            <script type="text/javascript">
            function seabird_start() {
                document.getElementById('seabird_image').style.opacity = "1";
            }
    
            function seabird_stop() {
                document.getElementById('seabird_image').style.opacity = "0";
            }
            seabird_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2403.20318">
            <span class="papertitle">SeaBird: Segmentation in Birdâ€™s View with Dice Loss Improves Monocular 3D Detection of Large Objects</span>
            </a>
            <br>
            Abhinav Kumar,
            <strong>Yuliang Guo</strong>,
            Xinyu Huang,
            Liu Ren,
            Xiaoming Liu
            <br>
            <strong>CVPR 2024</strong>
            <br>
            [<a href="https://arxiv.org/abs/2403.20318">paper</a>]
            [<a href="https://github.com/abhi1kumar/SeaBird">code</a>]
            <p></p>
            <p>
                A mathematical framework to prove that the dice loss leads to superior noise-robustness and model convergence for large objects compared to regression losses. A monocular 3D detection method integrated with bird-eye view segmentation. 
            </p>
        </td>
        </tr>
    

    <tr onmouseout="behindtheveil_stop()" onmouseover="behindtheveil_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='behindtheveil_image'><video  width=100% muted autoplay loop>
            <source src="TODO" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/Behind_the_veil.gif' width=100%>
            </div>
            <script type="text/javascript">
            function behindtheveil_start() {
                document.getElementById('behindtheveil_image').style.opacity = "1";
            }
    
            function behindtheveil_stop() {
                document.getElementById('behindtheveil_image').style.opacity = "0";
            }
            behindtheveil_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2404.03070">
            <span class="papertitle">Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion</span>
            </a>
            <br>
            Su Sun,
            Cheng Zhao,
            <strong>Yuliang Guo</strong>,
            Ruoyu Wang,
            Xinyu Huang,
            Victor(Yingjie) Chen,
            Liu Ren
            <br>
            <strong>CVPR 2024</strong>
            <br>
            [<a href="https://arxiv.org/abs/2404.03070">paper</a>]
            [<a href="https://github.com/BoschRHI3NA/3D-CRS-dataset">project page</a>]
            <p></p>
            <p>
                The first neural reconstruction method able to complete the occluded surfaces from large scenes. A key enabler to build interactable environments from real world, generalizing robotic reinforcement learning via reduced domain gap.
            </p>
        </td>
        </tr>

    
    <tr onmouseout="copypaste_stop()" onmouseover="copypaste_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='copypaste_image'><video  width=100% muted autoplay loop>
            <source src="images/3D-copy-paste.gif" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/3D-copy-paste.gif' width=105%>
            </div>
            <script type="text/javascript">
            function copypaste_start() {
                document.getElementById('copypaste_image').style.opacity = "1";
            }
    
            function copypaste_stop() {
                document.getElementById('copypaste_image').style.opacity = "0";
            }
            copypaste_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://gyhandy.github.io/3D-Copy-Paste/">
            <span class="papertitle">3D Copy-Paste: Physically-Plausible Object Insertion for Monocular 3D Detection</span>
            </a>
            <br>
            Yuhao Ge,
            Hong-Xing Yu,
            Cheng Zhao,
            <strong>Yuliang Guo</strong>,
            Xinyu Huang,
            Liu Ren,
            Laurent Itti,
            Jiajun Wu
            <br>
            <strong>NeurIPS 2023</strong>
            <br>
            [<a href="https://arxiv.org/abs/2312.05277">paper</a>]
            [<a href="https://github.com/gyhandy/3D-Copy-Paste">code</a>]
            [<a href="https://gyhandy.github.io/3D-Copy-Paste/">project page</a>]
            <p></p>
            <p>
                A physically plausible indoor 3D object insertion approach to automatically "copy" virtual objects and "paste" them into real scenes.
            </p>
        </td>
        </tr>

    <tr onmouseout="suoslam_stop()" onmouseover="suoslam_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='suoslam_image'><video  width=100% muted autoplay loop>
            <source src="images/suo_slam_demo.gif" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/suo_slam_demo.gif' width=100%>
            <img src='images/suo_slam_teaser.png' width=100%>
            </div>
            <script type="text/javascript">
            function suoslam_start() {
                document.getElementById('suoslam_image').style.opacity = "1";
            }
    
            function suoslam_stop() {
                document.getElementById('suoslam_image').style.opacity = "0";
            }
            suoslam_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://github.com/rpng/suo_slam">
            <span class="papertitle">Symmetry and Uncertainty-Aware Object SLAM for 6DoF Object Pose Estimation</span>
            </a>
            <br>
            Nathaniel Merrill,
            <strong>Yuliang Guo</strong>,
            Xingxing Zuo,
            Xinyu Huang,
            Stefan Leutenegger,
            Xi Peng,
            Liu Ren,
            Guoquan Huang
            <br>
            <strong>CVPR 2022</strong>
            <br>
            [<a href="https://arxiv.org/abs/2205.01823">paper</a>]
            [<a href="https://github.com/rpng/suo_slam">code</a>]
            <p></p>
            <p>
                A keypoint-based object-level SLAM framework that can provide globally consistent 6DoF pose estimates for symmetric and asymmetric objects.
            </p>
        </td>
        </tr>

    
    <tr onmouseout="omnifusion_stop()" onmouseover="omnifusion_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='omnifusion_image'><video  width=100% muted autoplay loop>
            <source src="images/OmniFusion.gif" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/OmniFusion.gif' width=105%>
            </div>
            <script type="text/javascript">
            function omnifusion_start() {
                document.getElementById('omnifusion_image').style.opacity = "1";
            }
    
            function omnifusion_stop() {
                document.getElementById('omnifusion_image').style.opacity = "0";
            }
            omnifusion_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://github.com/yuyanli0831/OmniFusion">
            <span class="papertitle">OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion</span>
            </a>
            <br>
            Yuyan Li,
            <strong>Yuliang Guo</strong>,
            Zhixin Yan,
            Xinyu Huang,
            Ye Duan,
            Liu Ren,
            <br>
            <strong>CVPR 2022 (Oral Presentation)</strong>
            <br>
            [<a href="https://arxiv.org/abs/2203.00838">paper</a>]
            [<a href="https://github.com/yuyanli0831/OmniFusion">code</a>]
            <p></p>
            <p>
                The first transformer approach to handle 360 monocular depth estimation with spherical distortion. Novel designs include tangent-image coordinate embedding and geometry-aware feature fusion. 
            </p>
        </td>
        </tr>

    
    <tr onmouseout="popnet_stop()" onmouseover="popnet_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='popnet_image'><video  width=100% muted autoplay loop>
            <source src="images/Popnet_demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/Popnet.png' width=100%>
            </div>
            <script type="text/javascript">
            function popnet_start() {
                document.getElementById('popnet_image').style.opacity = "1";
            }
    
            function popnet_stop() {
                document.getElementById('popnet_image').style.opacity = "0";
            }
            popnet_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://github.com/oppo-us-research/PoP-Net">
            <span class="papertitle">PoP-Net: Pose over Parts Network for Multi-Person 3D Pose Estimation from a Depth Image</span>
            </a>
            <br>
            <strong>Yuliang Guo</strong>,
            Zhong Li,
            Zekun Li,
            Xiangyu Du,
            Shuxue Quan,
            Yi Xu,
            <br>
            <strong>WACV 2022</strong>
            <br>
            [<a href="https://arxiv.org/abs/2012.06734">paper</a>]
            [<a href="https://github.com/oppo-us-research/PoP-Net">code</a>]
            [<a href="https://github.com/oppo-us-research/PoP-Net">dataset</a>]
            <p></p>
            <p>
                A real-time method to predict multi-person 3D poses from a depth image. Introduce new part-level representation to enables an explicit fusion process of bottom-up part detection and global pose detection. A new 3D human posture dataset with challenging multi-person occlusion.
            </p>
        </td>
        </tr>


    <tr onmouseout="genlanenet_stop()" onmouseover="genlanenet_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='genlanenet_image'><video  width=100% muted autoplay loop>
            <source src="images/GenLaneNet.png" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/GenLaneNet.png' width=95%>
            </div>
            <script type="text/javascript">
            function genlanenet_start() {
                document.getElementById('genlanenet_image').style.opacity = "1";
            }
    
            function genlanenet_stop() {
                document.getElementById('genlanenet_image').style.opacity = "0";
            }
            genlanenet_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://github.com/yuliangguo/Pytorch_Generalized_3D_Lane_Detection">
            <span class="papertitle">Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection</span>
            </a>
            <br>
            <strong>Yuliang Guo</strong>,
            GuanGuang Chen, Peitao Zhao, Weide Zhang, Jinghao Miao, Jingao Wang, Tae Eun Choe
            <br>
            <strong>ECCV 2020</strong>
            <br>
            [<a href="https://arxiv.org/abs/2003.10656">paper</a>]
            [<a href="https://github.com/yuliangguo/Pytorch_Generalized_3D_Lane_Detection">code</a>]
            [<a href="https://github.com/yuliangguo/3D_Lane_Synthetic_Dataset">dataset</a>]
            <p></p>
            <p>
                A pioneer work in predicting 3D lanes from a single image with high generalization to novel scenes. A 3D lane synthetic dataset is introduced.
        </td>
        </tr>
    

    <tr onmouseout="diffgeo_stop()" onmouseover="diffgeo_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='diffgeo_image'><video  width=100% muted autoplay loop>
            <source src="images/diffgeo.png" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/DiffGeo_A.png' width=90%>
            <img src='images/DiffGeo_B.png' width=90%>
            </div>
            <script type="text/javascript">
            function diffgeo_start() {
                document.getElementById('diffgeo_image').style.opacity = "1";
            }
    
            function diffgeo_stop() {
                document.getElementById('diffgeo_image').style.opacity = "0";
            }
            diffgeo_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://github.com/yuliangguo/Differential_Geometry_in_Edge_Detection">
            <span class="papertitle">Differential Geometry in Edge Detection: Accurate Estimation of Position, Orientation and Curvature</span>
            </a>
            <br>
            Benjamin B. Kimia,
            Xiaoyan Li,
            <strong>Yuliang Guo</strong>,
            Amir Tamrakar
            <br>
            <strong>TPAMI 2018</strong>
            <br>
            [<a href="https://d1wqtxts1xzle7.cloudfront.net/97699503/Tpami.2018.284626820230123-1-9kv4cc.pdf?1674495976=&response-content-disposition=inline%3B+filename%3DDifferential_Geometry_in_Edge_Detection.pdf&Expires=1710221673&Signature=cKAkuzIBnNHTMMX1B3WCjPPK7cxarVPHlT2iHaZI4SOKYmIPd5I1LeN0aUQT-gGVyFv9BHl1s6EBU~YfyKxBh2264RLa71mfwfLav2FXQtbdUH2E2pc5Zkqqm8M0qdFMBCOCeeALccEg7cL6Gxl-U8HJNRg-P8ZJ4n~-YqcKP4ozCo8JhmHVDg9OTIkWUyky6VrOuy0kW~gFRkpOAxfNrEN8x0GJOMTcB22zX-zhv4ex2Z5Jq4nmZsYtfTYpNAReFOCRL-HRhe6IqKFg7~BGmY7jY9b~NFM~8pQ-XmNlukwPQJAWJ6z9YF4ASrivy037x81cVboMrJZc5y1T5UFiIg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA">paper</a>]
            [<a href="https://github.com/yuliangguo/Differential_Geometry_in_Edge_Detection">code</a>]
            [<a href="https://drive.google.com/file/d/1r0s-FLaIrlnlW0Ue-HZEnScP3C4__CEX/view?usp=drive_link">dataset</a>]
            <p></p>
            <p>
                Numerically robust techniques to precisely estimate differential geometry attributes associated with image edges, including localization, orientation, and curvature, as well as edge topology. A curve fragment dataset is introduced for the evaluation of precise geometric attributes. 
        </td>
        </tr>


    <tr onmouseout="poseshapetrack_stop()" onmouseover="poseshapetrack_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='poseshapetrack_image'><video  width=100% muted autoplay loop>
            <source src="images/PoseShapeTrack.png" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/PoseShapeTrack.png' width=100%>
            </div>
            <script type="text/javascript">
            function poseshapetrack_start() {
                document.getElementById('poseshapetrack_image').style.opacity = "1";
            }
    
            function poseshapetrack_stop() {
                document.getElementById('poseshapetrack_image').style.opacity = "0";
            }
            poseshapetrack_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://arxiv.org/abs/1806.11011">
            <span class="papertitle">Robust Pose Tracking with a Joint Model of Appearance and Shape</span>
            </a>
            <br>
            <strong>Yuliang Guo</strong>,
            Lakshmi N. Govindarajan, Benjamin B. Kimia, Thomas Serre            
            <br>
            <em>arXiv</em> 2018
            <br>
            [<a href="https://arxiv.org/abs/1806.11011">paper</a>]
            <p></p>
            <p>
                A novel approach for estimating the 2D pose of an articulated object with an application to automated video analysis of small laboratory animals.
        </td>
        </tr>


    <tr onmouseout="BoMW_stop()" onmouseover="BoMW_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='BoMW_image'><video  width=100% muted autoplay loop>
            <source src="images/BoMW.png" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/BoMW.png' width=100%>
            </div>
            <script type="text/javascript">
            function BoMW_start() {
                document.getElementById('BoMW_image').style.opacity = "1";
            }
    
            function BoMW_stop() {
                document.getElementById('BoMW_image').style.opacity = "0";
            }
            BoMW_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://pureadmin.qub.ac.uk/ws/files/131264882/main_gesture_final.pdf">
            <span class="papertitle">Robust Pose Tracking with a Joint Model of Appearance and Shape</span>
            </a>
            <br>
            Lei Zhang, Shengping Zhang, Feng Jiang, Yuankai Qi, Jun Zhang, <strong>Yuliang Guo</strong>, Huiyu Zhou
            <br>
            <em>IEEE Transactions on Circuits and Systems for Video Technology 2017</em>
            <br>
            [<a href="https://pureadmin.qub.ac.uk/ws/files/131264882/main_gesture_final.pdf">Paper</a>]
            <p></p>
            <p>
                One-shot learning gesture recognition on RGB-D data recorded from Microsoft Kinect. A novel bag of manifold words (BoMW) based feature representation on sysmetric positive definite (SPD) manifolds.
        </td>
        </tr>

    <tr onmouseout="brycue_stop()" onmouseover="brycue_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='brycue_image'><video  width=100% muted autoplay loop>
            <source src="images/BryCue.png" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/BryCue.png' width=100%>
            </div>
            <script type="text/javascript">
            function brycue_start() {
                document.getElementById('brycue_image').style.opacity = "1";
            }
    
            function brycue_stop() {
                document.getElementById('brycue_image').style.opacity = "0";
            }
            brycue_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://www.sciencedirect.com/science/article/pii/S0042698915003685">
            <span class="papertitle">A Systematic Comparison between Visual Cues for Boundary Detection</span>
            </a>
            <br>
            David A. Mely, Junkyung Kim, Mason McGill, <strong>Yuliang Guo</strong>, Thomas Serre
            <br>
            <strong>Vision Research 2016</strong>
            <br>
            [<a href="https://www.sciencedirect.com/science/article/pii/S0042698915003685">paper</a>]
            [<a href="https://serre-lab.clps.brown.edu/resource/multicue/">dataset</a>]
            <p></p>
            <p>
                This study investigates the relative diagnosticity and the optimal combination of multiple cues (we consider luminance, color, motion and binocular disparity) for boundary detection in natural scenes. A multi-cue boundary dataset is introduced to facilitate the study.
        </td>
        </tr>


    <tr onmouseout="multistagecurve_stop()" onmouseover="multistagecurve_start()">
        <td style="padding:20px;width:40%;vertical-align:middle">
            <div class="one">
            <div class="two" id='multistagecurve_image'><video  width=100% muted autoplay loop>
            <source src="images/MultiStageCurve.png" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <!-- <img src='images/MultiStageCurve.png' width=100%> -->
            <img src='images/MultiStageCurve2.png' width=95%>
            </div>
            <script type="text/javascript">
            function multistagecurve_start() {
                document.getElementById('multistagecurve_image').style.opacity = "1";
            }
    
            function multistagecurve_stop() {
                document.getElementById('multistagecurve_image').style.opacity = "0";
            }
            multistagecurve_stop()
            </script>
        </td>
        <td style="padding:20px;width:60%;vertical-align:middle">
            <a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ECCV-2014/papers/8689/86890663.pdf">
            <span class="papertitle">A Multi-Stage Approach to Curve Extraction</span>
            </a>
            <br>
            <strong>Yuliang Guo</strong>, Naman Kumar, Maruthi Narayanan, Benjamin B Kimia
            <br>
            <strong>ECCV 2014</strong>
            <br>
            [<a href="https://projet.liris.cnrs.fr/imagine/pub/proceedings/ECCV-2014/papers/8689/86890663.pdf">paper</a>]
            [<a href="https://github.com/yuliangguo/MSEL_contour_extraction_cxx">code</a>]
            <p></p>
            <p>
                A multi-stage approach to curve extraction where the curve fragment search space is iteratively reduced by removing unlikely candidates using geometric constrains, but without affecting recall, to a point where the application of an objective functional becomes appropriate.        
            </td>
        </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Selected Patents</h2>
                <p>
                    <li><u>Yuliang Guo</u>, Xinyu Huang, Liu Ren, Systems and methods for providing product assembly step recognition using augmented reality, US Patent 11,715,300, 2023</li>
                    <li><u>Yuliang Guo</u>, Xinyu Huang, Liu Ren, Semantic SLAM Framework for Improved Object Pose Estimation, US Patent App. 17/686,677, 2023</li>
                    <li><u>Yuliang Guo</u>, Zhixin Yan, Yuyan Li, Xinyu Huang, Liu Ren, Method for fast domain adaptation from perspective projection image domain to omnidirectional image domain in machine perception tasks, US Patent App. 17/545,673, 2023</li>
                    <li><u>Yuliang Guo</u>, Tae Eun Choe, KaWai Tsoi, Guang Chen, Weide Zhang, Determining vanishing points based on lane lines, US Patent 11,227,167, 2022</li>
                    <li>Tae Eun Choe, <u>Yuliang Guo</u>, Guang Chen, KaWai Tsoi, Weide Zhang, Sensor calibration system for autonomous driving vehicles, US Patent 10,891,747, 2021</li>

                </p>
            </td>
            </tr>
        </tbody></table>
    
  </body>
</html>