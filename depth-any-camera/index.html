<!DOCTYPE html>
<html>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=0.2">
  <meta charset="utf-8">
  <meta name="description"
        content="Depth Any Camera">
  <meta name="keywords" content="Monocular Depth Estimation">
  <title>Depth Any Camera</title>


  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="css/bulma.min.css">
  <link rel="stylesheet" href="css/bulma-carousel.min.css">
  <link rel="stylesheet" href="css/bulma-slider.min.css">
  <link rel="stylesheet" href="css/fontawesome.all.min.css">


  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="css/index.css">
  <link rel="icon" href="img/Logo.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="js/fontawesome.all.min.js"></script>
  <script src="js/bulma-carousel.min.js"></script>
  <script src="js/bulma-slider.min.js"></script>
  <script src="js/index.js"></script>
  <script src="js/script.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><img id="painting_icon" width="8%" src="img/logo.jpg" style="vertical-align: middle; margin-right: 0px; position: relative; top: -2px;"> Depth Any Camera</h1>
          <h1 class="title is-2 publication-title">Zero-Shot Metric Depth Estimation from Any Camera</h1>

          <div class="is-size-5 publication-authors">
            <br>
            <span class="author-block">
              <a href="https://yuliangguo.github.io/">Yuliang Guo</a><sup>1*&dagger;</sup></a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp  
            </span>
            <span class="author-block">
              <a href="https://sparsh913.github.io/sparshgarg/">Sparsh Garg</a><sup>2&dagger;</sup></a>&nbsp</a>&nbsp</a>&nbsp
            </span>
            <span class="author-block">
              <a href="https://miangoleh.github.io/">S. Mahdi H. Miangoleh</a><sup>3</sup></a>&nbsp</a>&nbsp</a>&nbsp
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=cL4bNBwAAAAJ&hl=en">Xinyu Huang</a><sup>1</sup></a>&nbsp</a>&nbsp</a>&nbsp
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/liurenshomepage/">Liu Ren</a><sup>1</sup>  
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Bosch Research North America</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp </span>
            <span class="author-block"><sup>2</sup>Carnegie Mellon University</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp </span>
            <span class="author-block"><sup>3</sup>Simon Fraser University</span>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block">* Corresponding author</span> </a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp</a>&nbsp
            <span class="author-block">&dagger; Equal technical contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="is-size-4 publication-authors">
              <span class="author-block"><b>CVPR 2025</b></span>
            </div>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2501.02464" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2501.02464" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/yuliangguo/depth_any_camera" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/yuliangguo/depth-any-camera/blob/main/zipnerf_dac_swinl_indoor_2025_01.zip" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Fisheye Depth ZipNeRF</span>
                </a>
                <a href="https://huggingface.co/yuliangguo/depth-any-camera/blob/main/scannetpp_dac_swinl_indoor_2025_01.zip" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-database"></i>
                </span>
                <span>Fisheye Depth ScanNet++</span>
              </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body is-centered">
      <img id="teaser" width="75%" class="center" src="img/teaser.png"/>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h4 class="subtitle has-text-centered">
        <div class="content has-text-justified"> 
          <p>
            <b>Depth Any Camera (DAC)</b> is a powerful Zero-Shot Metric Depth Estimation framework that extends a perspective-trained model to handle any type of camera with varying FoVs effectively. 
            Remarkably, DAC can be <b>trained exclusively on perspective images</b>, yet it generalizes seamlessly to <b>fisheye</b> and <b>360</b> cameras without requiring specialized training data. Key features includes:
            <ul type="1">
              <li><b>Zero-shot metric</b> depth estimation on <b>fisheye</b> and <b>360</b> images, significantly outperforming prior metric depth SoTA <a href="https://jugghm.github.io/Metric3Dv2/" target="_blank">Metric3D-v2</a> and <a href="https://github.com/lpiccinelli-eth/UniDepth/tree/main" target="_blank">UniDepth</a></span></li>
              <li>Geometry-focused training framework <b>adaptable to any network archetecture</b>, extendable to <b>other 3D perception tasks</b></span></li>
            </ul>
            Tired of collecting new data and annotations for every new camera type? DAC allows you to leverage existing data, ensuring that <b>every piece of previously collected 3D data valuable</b>, regardless of the camera type used in a new application.
          </p>
          <p>
          </p>
        </div>
      </h4>
      
    </div>
  </div>
</section>


<section class="section"  style="margin-top: 0; padding-top: 50px;">
    <h2 class="title is-4" style="text-align: center;"> Demonstrations on Fisheye Videos and 360<sup>o</sup> Singe-View Reconstruction</h2>
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <p>
            The zero-shot metric depth estimation results of Depth Any Camera (DAC) are visualized on ScanNet++ fisheye videos and compared to Metric3D-v2. The visualizations of A.Rel error against ground truth highlight the superior performance of DAC. 
            Additionally, we showcase DAC's application on 360-degree images, where a single forward pass of depth estimation enables full 3D scene reconstruction.
          </p><br>
          <video poster="" id="ade" autoplay controls muted loop playsinline height="100%">
            <source src="img/videos/video_scannet++_1.mov"
                    type="video/mp4">
          </video>
          <video poster="" id="ade" autoplay controls muted loop playsinline height="100%">
            <source src="img/videos/video_scannet++_2.mov"
                    type="video/mp4">
          </video>
          <video poster="" id="ade" autoplay controls muted loop playsinline height="100%">
            <source src="img/videos/video_matterport3d_1.mov"
                    type="video/mp4">
          </video>
          <video poster="" id="ade" autoplay controls muted loop playsinline height="100%">
            <source src="img/videos/video_matterport3d_2.mov"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </section>


<section class="section"  style="margin-top: 0; padding-top: 50px;">
  <h2 class="title is-4" style="text-align: center;"> Fisheye Depth Estimation on ZipNeRF for Neural Reconstruction</h2>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <p>
            To support the development of 3D Neural Reconstruction methods such as NeRF and Gaussian Splatting on fisheye inputs, we provide DAC's depth estimation results on ZipNeRF for fisheye images. 
            The depth maps are available for download at <a href="https://huggingface.co/yuliangguo/depth-any-camera/blob/main/zipnerf_dac_swinl_indoor_2025_01.zip" target="_blank">Fisheye Depth ZipNeRF</a> and <a href="https://huggingface.co/yuliangguo/depth-any-camera/blob/main/scannetpp_dac_swinl_indoor_2025_01.zip" target="_blank">Fisheye Depth ScanNet++</a>.
            Emerging neural reconstruction methods capable of handling large FoV inputs, such as <a href="https://smerf-3d.github.io/" target="_blank">SMERF</a>, <a href="https://github.com/zmliao/Fisheye-GS" target="_blank">Fisheye-GS</a>, and <a href="https://github.com/half-potato/ever_training" target="_blank">EVER</a>, are expected to benefit from the fisheye depth prior.
        </p><br>
        <div class="image-area" style="text-align: center;">
          <img src="img/videos/zipnerf_fisheye_alameda.gif" width="80%">
        </div>
        <div class="image-area" style="text-align: center;">
          <img src="img/videos/zipnerf_fisheye_berlin.gif" width="80%">
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-small" style="margin-top: 30px; padding-top: 0px;">
  <h2 class="title is-4" style="text-align: center;">Visual comparison to <a href="https://jugghm.github.io/Metric3Dv2/" target="_blank">Metric3D-v2</a> and <a href="https://github.com/lpiccinelli-eth/UniDepth/tree/main" target="_blank">UniDepth</a></h2>
  <div class="hero-body">
    
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <img src="img/dac_vis_web_1.png" alt="Steve GIF" width="100%">
        </div>

        <div class="item item-steve">
            <img src="img/dac_vis_web_2.png" alt="Steve GIF" width="100%">
        </div>

        <!-- <div class="item item-steve">
            <img src="img/dac_vis_web_3.png" alt="Steve GIF" width="100%">
        </div> -->

        <div class="item item-steve">
            <img src="img/dac_vis_web_4.png" alt="Steve GIF" width="100%">
        </div>
    </div>
    
  </div>
</section>


<!-- <section class="section" style="background-color:#efeff081"> -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While recent depth foundation models exhibit strong zero-shot generalization, achieving accurate metric depth across diverse camera types—particularly those with large fields of view (FoV) such as fisheye and 360-degree cameras—remains a significant challenge.
            This paper presents <strong>Depth Any Camera (DAC)</strong>, a powerful zero-shot metric depth estimation framework that extends a perspective-trained model to effectively handle cameras with varying FoVs. The framework is designed to ensure that all existing 3D data can be leveraged, regardless of the specific camera types used in new applications.
            Remarkably, DAC is trained exclusively on perspective images but generalizes seamlessly to fisheye and 360-degree cameras without the need for specialized training data. DAC employs Equi-Rectangular Projection (ERP) as a unified image representation, enabling consistent processing of images with diverse FoVs. 
            Its key components include a <b>pitch-aware Image-to-ERP conversion</b> for efficient online augmentation in ERP space, a <b>FoV alignment</b> operation to support effective training across a wide range of FoVs, and multi-resolution data augmentation to address resolution disparities between training and testing.
            DAC achieves state-of-the-art zero-shot metric depth estimation, improving delta-1 accuracy by up to <b>50%</b> on multiple fisheye and 360-degree datasets compared to prior metric depth foundation models, demonstrating robust generalization across camera types.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4" style="text-align: center;">Data Coverage</h2>
          <div class="content has-text-justified">
            <p>
              DAC is trained on a combination set of 3 labeled datasets (670k images) for indoor model and a combination of 2 datasets (130k) for outdoor model. Two 360 datasets and two fisheye datasets are used for zero-shot testing.
            </p>
          </div>

          <div class="image-area">
            <img src="img/table_data_coverage.png" alt="pipeline" width="100%" class="center">
          </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4" style="text-align: center;">Zero-shot Metric Depth Estimation</h2>
          <div class="content has-text-justified">
            <p>
              Depth Any Camera (DAC) performs <b>significantly better</b> than the previously SoTA <b>metric</b> depth estimation models Metric3D-v2 and  UniDepth in zero-shot generalization to large FoV camera images given <b>significantly smaller training dataset and model size</b>.
            </p>
          </div>

          <div class="image-area">
            <img src="img/table_compare_sota.png" alt="pipeline" width="100%">
          </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4" style="text-align: center;">Framework</h2>
        <div class="content has-text-justified">
          <p>
            The framework of Depth Any Camera is shown below. Our DAC framework converts data from any camera type into a canonical ERP space, enabling
            a model trained solely on perspective images to process large-FoV test data consistently for metric depth estimation. During training, we
            introduce an effective <b>pitch-aware Image-to-ERP conversion</b> with online data augmentation to simulate high-distortion regions unique to
            large-FoV images. The proposed <b>FoV-Align</b> process normalizes diverse-FoV data to a predefined ERP patch size, maximizing training
            efficiency. During inference, images from any camera type are converted into ERP space for depth estimation, with an optional step to map
            the ERP output back to the original image space for visualization.
          </p>
        </div>

        <div class="image-area">
          <img src="img/pipeline.png" alt="pipeline" width="100%">
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section pt-0" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title is-4">Citation</h2>
    <pre class="selectable"><code>@inproceedings{guo2025depthanycamera,
  title={Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera},
  author={Yuliang Guo and Sparsh Garg and S. Mahdi H. Miangoleh and Xinyu Huang and Liu Ren},
  booktitle={CVPR},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2501.02464">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/yuliangguo/depth_any_camera" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="content">
        <p>
          The template is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>
        </p>
      </div>
    </div>
  </div>
</footer>

</body>
</html>